{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10005875",
   "metadata": {},
   "source": [
    "# RAG Evaluation Lab\n",
    "\n",
    "This notebook implements a small, self-contained RAG evaluation framework:\n",
    "\n",
    "1. Build a synthetic document corpus and ground-truth Q&A set.\n",
    "2. Create embeddings and a vector index for retrieval.\n",
    "3. Evaluate retrieval quality with:\n",
    "   - Recall@k\n",
    "   - Precision@k\n",
    "   - MRR (Mean Reciprocal Rank)\n",
    "4. (Optional) Generate answers with an LLM using retrieved context.\n",
    "5. (Optional) Use an LLM-as-judge to evaluate answer correctness and faithfulness.\n",
    "\n",
    "You can adapt this structure to real datasets later. For now, the goal is to demonstrate *evaluation thinking* around RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38208fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Uncomment if you want to use OpenAI embeddings / LLM-as-judge\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# Optional: use SentenceTransformers locally instead of OpenAI embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    HAS_SENTENCE_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_SENTENCE_TRANSFORMERS = False\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# If you use OpenAI:\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY in your environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8594fc",
   "metadata": {},
   "source": [
    "## 1. Define a synthetic document corpus\n",
    "\n",
    "We’ll create a small, focused corpus that simulates developer documentation for an internal tool or API.\n",
    "Each document has an `id` and `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc35bd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>The Developer Telemetry API collects events su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>To enable code generation in the IDE, develope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>Recall@k is a retrieval evaluation metric that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>The embeddings service supports multiple model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>Latency budgets for AI-assisted coding should ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text\n",
       "0  doc_1  The Developer Telemetry API collects events su...\n",
       "1  doc_2  To enable code generation in the IDE, develope...\n",
       "2  doc_3  Recall@k is a retrieval evaluation metric that...\n",
       "3  doc_4  The embeddings service supports multiple model...\n",
       "4  doc_5  Latency budgets for AI-assisted coding should ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"text\": (\n",
    "            \"The Developer Telemetry API collects events such as suggestion_accept, \"\n",
    "            \"suggestion_reject, and compile_error. Events are batched and sent every 5 seconds.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\",\n",
    "        \"text\": (\n",
    "            \"To enable code generation in the IDE, developers must install the Codex Assistant plugin \"\n",
    "            \"and authenticate using their API key in the settings panel.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"text\": (\n",
    "            \"Recall@k is a retrieval evaluation metric that measures how often at least one \"\n",
    "            \"relevant document appears in the top-k results for a query.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_4\",\n",
    "        \"text\": (\n",
    "            \"The embeddings service supports multiple models. The default model is optimized for \"\n",
    "            \"semantic search over short text like code comments, function names, and log messages.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_5\",\n",
    "        \"text\": (\n",
    "            \"Latency budgets for AI-assisted coding should keep P95 completion under 500 milliseconds \"\n",
    "            \"to avoid interrupting the developer's flow.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "docs_df = pd.DataFrame(documents)\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645af68",
   "metadata": {},
   "source": [
    "## 2. Define ground-truth Q&A pairs\n",
    "\n",
    "Each question has:\n",
    "- `question`\n",
    "- `answer`\n",
    "- `relevant_docs`: list of document IDs that contain the necessary information.\n",
    "\n",
    "This lets us compute retrieval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb184c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevant_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q_1</td>\n",
       "      <td>Which events does the Developer Telemetry API ...</td>\n",
       "      <td>It collects suggestion_accept, suggestion_reje...</td>\n",
       "      <td>[doc_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q_2</td>\n",
       "      <td>How does a developer enable code generation in...</td>\n",
       "      <td>Install the Codex Assistant plugin and authent...</td>\n",
       "      <td>[doc_2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q_3</td>\n",
       "      <td>What is Recall@k in retrieval evaluation?</td>\n",
       "      <td>Recall@k measures how often at least one relev...</td>\n",
       "      <td>[doc_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q_4</td>\n",
       "      <td>What is the default embeddings model optimized...</td>\n",
       "      <td>Semantic search over short text like code comm...</td>\n",
       "      <td>[doc_4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q_5</td>\n",
       "      <td>What should P95 latency be for AI-assisted cod...</td>\n",
       "      <td>Under 500 milliseconds so the developer's flow...</td>\n",
       "      <td>[doc_5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           question  \\\n",
       "0  q_1  Which events does the Developer Telemetry API ...   \n",
       "1  q_2  How does a developer enable code generation in...   \n",
       "2  q_3          What is Recall@k in retrieval evaluation?   \n",
       "3  q_4  What is the default embeddings model optimized...   \n",
       "4  q_5  What should P95 latency be for AI-assisted cod...   \n",
       "\n",
       "                                              answer relevant_docs  \n",
       "0  It collects suggestion_accept, suggestion_reje...       [doc_1]  \n",
       "1  Install the Codex Assistant plugin and authent...       [doc_2]  \n",
       "2  Recall@k measures how often at least one relev...       [doc_3]  \n",
       "3  Semantic search over short text like code comm...       [doc_4]  \n",
       "4  Under 500 milliseconds so the developer's flow...       [doc_5]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs = [\n",
    "    {\n",
    "        \"id\": \"q_1\",\n",
    "        \"question\": \"Which events does the Developer Telemetry API collect?\",\n",
    "        \"answer\": \"It collects suggestion_accept, suggestion_reject, and compile_error events.\",\n",
    "        \"relevant_docs\": [\"doc_1\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q_2\",\n",
    "        \"question\": \"How does a developer enable code generation in the IDE?\",\n",
    "        \"answer\": \"Install the Codex Assistant plugin and authenticate with their API key in settings.\",\n",
    "        \"relevant_docs\": [\"doc_2\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q_3\",\n",
    "        \"question\": \"What is Recall@k in retrieval evaluation?\",\n",
    "        \"answer\": \"Recall@k measures how often at least one relevant document appears in the top-k results.\",\n",
    "        \"relevant_docs\": [\"doc_3\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q_4\",\n",
    "        \"question\": \"What is the default embeddings model optimized for?\",\n",
    "        \"answer\": \"Semantic search over short text like code comments, function names, and log messages.\",\n",
    "        \"relevant_docs\": [\"doc_4\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q_5\",\n",
    "        \"question\": \"What should P95 latency be for AI-assisted coding?\",\n",
    "        \"answer\": \"Under 500 milliseconds so the developer's flow is not interrupted.\",\n",
    "        \"relevant_docs\": [\"doc_5\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "qa_df = pd.DataFrame(qa_pairs)\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e618f",
   "metadata": {},
   "source": [
    "## 3. Create embeddings for documents and questions\n",
    "\n",
    "You can use either:\n",
    "\n",
    "- **SentenceTransformers** (local, no API key), or  \n",
    "- **OpenAI embeddings** (commented out here, easy to switch in).\n",
    "\n",
    "For this lab, SentenceTransformers is a good default if installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40b8bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformers not found. Falling back to random embeddings (for structure only).\n",
      "Install with: pip install sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 384  # default for all-MiniLM-L6-v2\n",
    "\n",
    "if HAS_SENTENCE_TRANSFORMERS:\n",
    "    print(\"Using SentenceTransformers for embeddings.\")\n",
    "    st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "        return st_model.encode(texts, show_progress_bar=False)\n",
    "else:\n",
    "    print(\"SentenceTransformers not found. Falling back to random embeddings (for structure only).\")\n",
    "    print(\"Install with: pip install sentence-transformers\")\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "        # This is just a structural placeholder. Replace with real embeddings.\n",
    "        return np.random.normal(size=(len(texts), EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "# If you want to use OpenAI embeddings instead, you could do:\n",
    "#\n",
    "# def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "#     response = client.embeddings.create(\n",
    "#         model=\"text-embedding-3-small\",\n",
    "#         input=texts,\n",
    "#     )\n",
    "#     return np.array([item.embedding for item in response.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f537238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = embed_texts(docs_df[\"text\"].tolist())\n",
    "doc_embeddings.shape\n",
    "\n",
    "# Build a simple nearest-neighbor index using cosine similarity\n",
    "nn_model = NearestNeighbors(\n",
    "    n_neighbors=5,\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "nn_model.fit(doc_embeddings)\n",
    "\n",
    "doc_id_to_index = {doc_id: i for i, doc_id in enumerate(docs_df[\"id\"].tolist())}\n",
    "index_to_doc_id = {i: doc_id for doc_id, i in doc_id_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b096d",
   "metadata": {},
   "source": [
    "## 4. Define a retrieval function\n",
    "\n",
    "Given a question, we embed it and retrieve the top-k most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49c8b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_1 Which events does the Developer Telemetry API collect?\n",
      "[('doc_3', 0.03358895374010673), ('doc_4', -0.02766602495166426), ('doc_1', -0.043924410386471546)]\n",
      "-----\n",
      "q_2 How does a developer enable code generation in the IDE?\n",
      "[('doc_5', 0.008848852463883006), ('doc_2', 0.006211998633499194), ('doc_1', -0.012547493967877132)]\n",
      "-----\n",
      "q_3 What is Recall@k in retrieval evaluation?\n",
      "[('doc_2', 0.0916082885693893), ('doc_5', 0.0025658563007698865), ('doc_3', -0.015039476951177688)]\n",
      "-----\n",
      "q_4 What is the default embeddings model optimized for?\n",
      "[('doc_3', 0.05250199903712438), ('doc_1', 0.04694633891020672), ('doc_2', -0.0015075832319872973)]\n",
      "-----\n",
      "q_5 What should P95 latency be for AI-assisted coding?\n",
      "[('doc_5', 0.08730096197955117), ('doc_2', 0.050156622319949884), ('doc_3', 0.015292664700832237)]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k(question: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "    q_vec = embed_texts([question])\n",
    "    distances, indices = nn_model.kneighbors(q_vec, n_neighbors=k)\n",
    "    distances = distances[0]\n",
    "    indices = indices[0]\n",
    "\n",
    "    # cosine distance -> similarity (1 - distance)\n",
    "    sims = 1 - distances\n",
    "\n",
    "    results = []\n",
    "    for idx, sim in zip(indices, sims):\n",
    "        doc_id = index_to_doc_id[idx]\n",
    "        results.append((doc_id, float(sim)))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "for q in qa_pairs:\n",
    "    print(q[\"id\"], q[\"question\"])\n",
    "    print(retrieve_top_k(q[\"question\"], k=3))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7b76a",
   "metadata": {},
   "source": [
    "## 5. Compute retrieval metrics\n",
    "\n",
    "We will compute:\n",
    "\n",
    "- **Recall@k** – fraction of queries where *any* relevant doc appears in the top-k.  \n",
    "- **Precision@k** – fraction of retrieved docs that are relevant.  \n",
    "- **MRR@k** – Mean Reciprocal Rank of first relevant doc in the top-k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e722b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Recall@3': 0.8,\n",
       "  'Precision@3': 0.26666666666666666,\n",
       "  'MRR@3': 0.6666666666666666},\n",
       " {'Recall@5': 1.0, 'Precision@5': 0.2, 'MRR@5': 0.33999999999999997})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_retrieval_metrics(\n",
    "    qa_df: pd.DataFrame,\n",
    "    k: int = 3\n",
    ") -> Dict[str, float]:\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for _, row in qa_df.iterrows():\n",
    "        q_id = row[\"id\"]\n",
    "        question = row[\"question\"]\n",
    "        relevant_docs = set(row[\"relevant_docs\"])\n",
    "\n",
    "        retrieved = retrieve_top_k(question, k=k)\n",
    "        retrieved_ids = [doc_id for doc_id, _ in retrieved]\n",
    "\n",
    "        # Recall@k: 1 if any relevant doc is in top-k, else 0\n",
    "        hit = int(bool(relevant_docs.intersection(retrieved_ids)))\n",
    "        recalls.append(hit)\n",
    "\n",
    "        # Precision@k: (# relevant in top-k) / k\n",
    "        num_rel = len(relevant_docs.intersection(retrieved_ids))\n",
    "        precisions.append(num_rel / k)\n",
    "\n",
    "        # MRR@k: 1/rank_of_first_relevant if found, else 0\n",
    "        rr = 0.0\n",
    "        for rank, doc_id in enumerate(retrieved_ids, start=1):\n",
    "            if doc_id in relevant_docs:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        reciprocal_ranks.append(rr)\n",
    "\n",
    "    return {\n",
    "        f\"Recall@{k}\": float(np.mean(recalls)),\n",
    "        f\"Precision@{k}\": float(np.mean(precisions)),\n",
    "        f\"MRR@{k}\": float(np.mean(reciprocal_ranks)),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics_k3 = compute_retrieval_metrics(qa_df, k=3)\n",
    "metrics_k5 = compute_retrieval_metrics(qa_df, k=5)\n",
    "metrics_k3, metrics_k5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983e98e",
   "metadata": {},
   "source": [
    "## 6. (Optional) Generate answers using an LLM\n",
    "\n",
    "For each question, we:\n",
    "1. Retrieve top-k documents.\n",
    "2. Build a context string from retrieved docs.\n",
    "3. Ask an LLM to answer the question using that context.\n",
    "\n",
    "> ⚠️ This step requires an OpenAI API key or another LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0a1d8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LLM generation. Set USE_LLM_GENERATION = True to enable.\n"
     ]
    }
   ],
   "source": [
    "USE_LLM_GENERATION = False  # Set to True if you want to run this and have an API key\n",
    "\n",
    "\n",
    "def build_context_from_docs(doc_ids: List[str]) -> str:\n",
    "    \"\"\"Build a text block from retrieved document IDs.\"\"\"\n",
    "    texts = []\n",
    "    for doc_id in doc_ids:\n",
    "        text = docs_df.loc[docs_df[\"id\"] == doc_id, \"text\"].values[0]\n",
    "        texts.append(f\"[{doc_id}] {text}\")\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "\n",
    "def generate_answer_with_llm(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Example with OpenAI client.\n",
    "    Replace with your actual LLM call as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example with OpenAI (commented out)\n",
    "    #\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"gpt-4o-mini\",\n",
    "    #     messages=[\n",
    "    #         {\n",
    "    #             \"role\": \"system\",\n",
    "    #             \"content\": (\n",
    "    #                 \"You are a helpful assistant that answers strictly based on \"\n",
    "    #                 \"the provided context. Do NOT hallucinate.\"\n",
    "    #             ),\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": (\n",
    "    #                 f\"Context:\\n{context}\\n\\n\"\n",
    "    #                 f\"Question: {question}\\n\"\n",
    "    #                 \"Answer briefly and accurately:\"\n",
    "    #             ),\n",
    "    #         },\n",
    "    #     ],\n",
    "    # )\n",
    "    # return response.choices[0].message.content.strip()\n",
    "\n",
    "    # Placeholder for offline use\n",
    "    return f\"(Mock answer) Based on the context, here's an answer to: {question}\"\n",
    "\n",
    "\n",
    "generated_answers = []\n",
    "\n",
    "if USE_LLM_GENERATION:\n",
    "    for _, row in qa_df.iterrows():\n",
    "        q_id = row[\"id\"]\n",
    "        question = row[\"question\"]\n",
    "\n",
    "        retrieved = retrieve_top_k(question, k=3)\n",
    "        retrieved_ids = [doc_id for doc_id, _ in retrieved]\n",
    "        context = build_context_from_docs(retrieved_ids)\n",
    "\n",
    "        answer = generate_answer_with_llm(question, context)\n",
    "\n",
    "        generated_answers.append(\n",
    "            {\n",
    "                \"id\": q_id,\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": answer,\n",
    "                \"retrieved_docs\": retrieved_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    gen_df = pd.DataFrame(generated_answers)\n",
    "    display(gen_df.head())\n",
    "else:\n",
    "    print(\"Skipping LLM generation. Set USE_LLM_GENERATION = True to enable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b641a7",
   "metadata": {},
   "source": [
    "## 7. (Optional) LLM-as-judge: correctness & faithfulness\n",
    "\n",
    "Given:\n",
    "- Question\n",
    "- Ground-truth answer\n",
    "- Generated answer\n",
    "- Retrieved context\n",
    "\n",
    "We can ask an LLM to rate:\n",
    "- **Correctness**: Does the generated answer match the ground truth?\n",
    "- **Faithfulness**: Is it supported by the retrieved context or hallucinated?\n",
    "\n",
    "This mirrors how many teams evaluate RAG systems today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ddaa1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LLM-as-judge evaluation. Enable USE_LLM_JUDGE and USE_LLM_GENERATION to run.\n"
     ]
    }
   ],
   "source": [
    "USE_LLM_JUDGE = False  # set to True if you're ready to call the LLM\n",
    "\n",
    "def score_answer_with_llm_judge(\n",
    "    question: str,\n",
    "    ground_truth: str,\n",
    "    generated_answer: str,\n",
    "    context: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Example prompt for LLM-as-judge. Returns parsed numeric scores.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are evaluating a RAG system.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Ground truth answer:\n",
    "{ground_truth}\n",
    "\n",
    "Generated answer:\n",
    "{generated_answer}\n",
    "\n",
    "Retrieved context:\n",
    "{context}\n",
    "\n",
    "Please answer in JSON with the following fields:\n",
    "- correctness: integer from 1 to 5\n",
    "- faithfulness: integer from 1 to 5\n",
    "- explanation: short text explanation\n",
    "\"\"\"\n",
    "    # Example with OpenAI:\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"gpt-4o-mini\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    # )\n",
    "    # raw = response.choices[0].message.content\n",
    "    # Here you'd parse JSON from `raw`.\n",
    "\n",
    "    # Mock result structure\n",
    "    return {\n",
    "        \"correctness\": 4,\n",
    "        \"faithfulness\": 4,\n",
    "        \"explanation\": \"Mocked score. Replace with real LLM judge call.\",\n",
    "    }\n",
    "\n",
    "\n",
    "judge_results = []\n",
    "\n",
    "if USE_LLM_JUDGE and USE_LLM_GENERATION:\n",
    "    for _, row in qa_df.iterrows():\n",
    "        q_id = row[\"id\"]\n",
    "        question = row[\"question\"]\n",
    "        ground_truth = row[\"answer\"]\n",
    "\n",
    "        gen_row = gen_df.loc[gen_df[\"id\"] == q_id].iloc[0]\n",
    "        generated_answer = gen_row[\"generated_answer\"]\n",
    "        retrieved_ids = gen_row[\"retrieved_docs\"]\n",
    "        context = build_context_from_docs(retrieved_ids)\n",
    "\n",
    "        scores = score_answer_with_llm_judge(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            generated_answer=generated_answer,\n",
    "            context=context,\n",
    "        )\n",
    "        scores[\"id\"] = q_id\n",
    "        judge_results.append(scores)\n",
    "\n",
    "    judge_df = pd.DataFrame(judge_results)\n",
    "    judge_df\n",
    "else:\n",
    "    print(\"Skipping LLM-as-judge evaluation. Enable USE_LLM_JUDGE and USE_LLM_GENERATION to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad4a73",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "- Defined a small but structured corpus and ground-truth Q&A pairs.\n",
    "- Built embeddings and a vector index for retrieval.\n",
    "- Computed retrieval metrics: Recall@k, Precision@k, MRR.\n",
    "- (Optionally) generated answers with an LLM using retrieved context.\n",
    "- (Optionally) structured an LLM-as-judge evaluation for correctness and faithfulness.\n",
    "\n",
    "Next steps to make this *production-grade*:\n",
    "\n",
    "- Swap synthetic data with real docs (API docs, internal knowledge base, etc.).\n",
    "- Replace mock embeddings with real models (OpenAI, Cohere, local SentenceTransformers).\n",
    "- Plug in real LLM calls for answer generation and judging.\n",
    "- Log results over time to detect regressions when models, prompts, or retrieval strategies change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
