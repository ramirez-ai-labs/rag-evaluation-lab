{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df02329",
   "metadata": {},
   "source": [
    "# RAG & LLM Evaluation Tutorial (Beginner ‚Üí Intermediate)\n",
    "\n",
    "This notebook walks through **core evaluation ideas** you‚Äôll see in real-world Retrieval-Augmented Generation (RAG) and LLM systems:\n",
    "\n",
    "- How to measure **retrieval quality** (Are we fetching the right documents?)\n",
    "- How to measure **similarity** between texts (using **TF‚ÄëIDF + cosine similarity**)\n",
    "- How to approximate **answer quality** with a simple overlap metric\n",
    "\n",
    "> üéØ **Goal:** By the end, you should be able to read evaluation tables like *Recall@k, Precision@k, similarity scores,* and understand what they mean and how to compute them in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4601d",
   "metadata": {},
   "source": [
    "## 1. What is RAG and Why Do We Evaluate?\n",
    "\n",
    "**Retrieval‚ÄëAugmented Generation (RAG)** =\n",
    "\n",
    "> Use a **retriever** to fetch relevant documents ‚Üí feed them into an **LLM** ‚Üí generate an answer that is grounded in those docs.\n",
    "\n",
    "There are **two big pieces** to evaluate:\n",
    "\n",
    "1. üîé **Retrieval** ‚Äì did we fetch the *right* context?\n",
    "   - If retrieval is bad, even a great LLM will hallucinate or guess.\n",
    "\n",
    "2. üß† **Generation (LLM answers)** ‚Äì did the model:\n",
    "   - Use the retrieved information?\n",
    "   - Answer the actual question correctly?\n",
    "   - Avoid hallucinations?\n",
    "\n",
    "In this notebook, we‚Äôll build a **tiny, synthetic RAG setup** and focus on:\n",
    "\n",
    "- Retrieval metrics: **Recall@k, Precision@k**\n",
    "- Similarity scores: **TF‚ÄëIDF + cosine similarity**\n",
    "- Simple answer quality: **word overlap score**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e909895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports ready.\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Text / retrieval utilities\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Optional visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "\n",
    "print(\"‚úÖ Imports ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f3569",
   "metadata": {},
   "source": [
    "## 2. Tiny Synthetic RAG Dataset\n",
    "\n",
    "To keep things **simple and visible**, we‚Äôll build a tiny ‚Äúdoc + query‚Äù set in memory.\n",
    "\n",
    "- 5 documents (our *knowledge base*)\n",
    "- 3 user queries\n",
    "- For each query we define:\n",
    "  - Which docs are **truly relevant** (our *ground truth*)\n",
    "  - A **gold answer** (what a perfect assistant would say)\n",
    "  - A **model answer** (what our pretend LLM answered)\n",
    "\n",
    "In real systems, these would come from logs, annotation pipelines, or labeling tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881437c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n",
      "Number of queries: 3\n"
     ]
    }
   ],
   "source": [
    "# Our tiny \"corpus\" of documents (knowledge base)\n",
    "documents = [\n",
    "    \"RAG systems combine retrieval and generation using large language models.\",\n",
    "    \"TF-IDF is a classic method to turn documents into numerical vectors based on word frequency.\",\n",
    "    \"Vector databases store embeddings and let you search by semantic similarity.\",\n",
    "    \"Precision and recall are standard information retrieval metrics used to evaluate search quality.\",\n",
    "    \"Evaluation of LLMs often includes answer correctness, grounding, and hallucination analysis.\"\n",
    "]\n",
    "\n",
    "# Give each doc an ID for reference\n",
    "doc_ids = list(range(len(documents)))\n",
    "\n",
    "# Our tiny set of user questions (queries)\n",
    "queries = [\n",
    "    \"How do RAG systems work?\",\n",
    "    \"What is TF-IDF in information retrieval?\",\n",
    "    \"How do we evaluate search with precision and recall?\",\n",
    "]\n",
    "\n",
    "# For each query, define the doc indices that we consider truly relevant\n",
    "ground_truth_relevant = {\n",
    "    0: [0, 4],   # RAG question: docs 0 and 4 are relevant\n",
    "    1: [1],      # TF-IDF question: doc 1 is relevant\n",
    "    2: [3],      # precision/recall question: doc 3 is relevant\n",
    "}\n",
    "\n",
    "# Optional: gold answers (ideal) and model answers (simulated)\n",
    "gold_answers = {\n",
    "    0: \"A RAG system retrieves relevant documents and feeds them into an LLM to generate grounded answers.\",\n",
    "    1: \"TF-IDF is a way to score words by how frequent they are in a document versus across the whole corpus.\",\n",
    "    2: \"Search is evaluated by precision and recall, which look at how many retrieved results are relevant and how many relevant results were retrieved.\"\n",
    "}\n",
    "\n",
    "model_answers = {\n",
    "    0: \"RAG uses search plus a language model. It looks up context and then the LLM uses it to answer.\",\n",
    "    1: \"In TF-IDF, terms that appear often in a document but not everywhere get higher scores.\",\n",
    "    2: \"Precision and recall are metrics. Precision is about retrieved relevant things, recall is about how much you got.\"\n",
    "}\n",
    "\n",
    "print(\"Number of documents:\", len(documents))\n",
    "print(\"Number of queries:\", len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c5d86",
   "metadata": {},
   "source": [
    "## 3. Retrieval Metrics: Recall@k and Precision@k\n",
    "\n",
    "When we ask a question, the retriever returns a ranked list of documents.\n",
    "\n",
    "Two **very common metrics** are:\n",
    "\n",
    "### üîÅ Recall@k (coverage)\n",
    "\n",
    "> ‚ÄúOut of all the documents that *should* have been retrieved, how many did we actually retrieve in the top **k** results?‚Äù\n",
    "\n",
    "- High recall means: *we‚Äôre not missing relevant docs*.\n",
    "- Formula (for one query):  \n",
    "  \\( \\text{Recall@k} = \\frac{\\text{# relevant docs in top k}}{\\text{# relevant docs in ground truth}} \\)\n",
    "\n",
    "### üéØ Precision@k (accuracy of top‚Äëk)\n",
    "\n",
    "> ‚ÄúOut of the top **k** documents we retrieved, how many are actually relevant?‚Äù\n",
    "\n",
    "- High precision means: *most of what we show is good*.\n",
    "- Formula (for one query):  \n",
    "  \\( \\text{Precision@k} = \\frac{\\text{# relevant docs in top k}}{k} \\)\n",
    "\n",
    "These are simple but incredibly powerful metrics for understanding **retrieval quality**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0359f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@2 example: 0.5\n",
      "Precision@2 example: 0.5\n"
     ]
    }
   ],
   "source": [
    "def recall_at_k(retrieved: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"Compute Recall@k for a single query.\n",
    "    \n",
    "    retrieved: list of doc IDs in ranked order\n",
    "    relevant: list of ground-truth relevant doc IDs\n",
    "    k: how many top results to look at\n",
    "    \"\"\"\n",
    "    top_k = retrieved[:k]\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    hits = sum(1 for d in top_k if d in relevant)\n",
    "    return hits / len(relevant)\n",
    "\n",
    "\n",
    "def precision_at_k(retrieved: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"Compute Precision@k for a single query.\"\"\"\n",
    "    top_k = retrieved[:k]\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    hits = sum(1 for d in top_k if d in relevant)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Recall@2 example:\", recall_at_k([0, 1, 2], [0, 4], k=2))\n",
    "print(\"Precision@2 example:\", precision_at_k([0, 1, 2], [0, 4], k=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969a37b",
   "metadata": {},
   "source": [
    "### 3.1 Simple Keyword Retrieval (Baseline)\n",
    "\n",
    "First, we‚Äôll build a **very simple retriever**:\n",
    "\n",
    "- Count how many words in the query appear in each document.\n",
    "- Rank documents by that overlap count.\n",
    "\n",
    "This is **not** how real systems work in production, but it‚Äôs a great baseline to compare against more advanced methods like TF‚ÄëIDF or embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad19292c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>recall@3</th>\n",
       "      <th>precision@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  recall@3  precision@3\n",
       "0         0       0.5     0.333333\n",
       "1         1       1.0     0.333333\n",
       "2         2       1.0     0.333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Very simple tokenizer: lowercase + split on non-letters.\"\"\"\n",
    "    tokens = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def keyword_retrieve(query: str, docs: List[str]) -> List[int]:\n",
    "    \"\"\"Rank docs by how many query words they contain (descending).\"\"\"\n",
    "    q_tokens = tokenize(query)\n",
    "    doc_scores = []\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        d_tokens = tokenize(doc)\n",
    "        overlap = len(set(q_tokens) & set(d_tokens))\n",
    "        doc_scores.append((doc_id, overlap))\n",
    "    # Sort by score (descending), then doc_id for stability\n",
    "    ranked = sorted(doc_scores, key=lambda x: (-x[1], x[0]))\n",
    "    return [doc_id for doc_id, score in ranked]\n",
    "\n",
    "\n",
    "# Run keyword retrieval for each query and compute metrics\n",
    "results_keyword = []\n",
    "\n",
    "for qi, q in enumerate(queries):\n",
    "    ranked_docs = keyword_retrieve(q, documents)\n",
    "    relevant = ground_truth_relevant[qi]\n",
    "    r_at_3 = recall_at_k(ranked_docs, relevant, k=3)\n",
    "    p_at_3 = precision_at_k(ranked_docs, relevant, k=3)\n",
    "    results_keyword.append({\n",
    "        \"query_id\": qi,\n",
    "        \"query\": q,\n",
    "        \"ranked_docs\": ranked_docs,\n",
    "        \"recall@3\": r_at_3,\n",
    "        \"precision@3\": p_at_3,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results_keyword)[[\"query_id\", \"recall@3\", \"precision@3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f78dce",
   "metadata": {},
   "source": [
    "## 4. TF‚ÄëIDF + Cosine Similarity (Classic Information Retrival)\n",
    "\n",
    "Many search and RAG pipelines still use **TF‚ÄëIDF** as a baseline.\n",
    "\n",
    "### üßÆ What is TF‚ÄëIDF? (Beginner Explanation)\n",
    "\n",
    "**TF‚ÄëIDF** stands for:\n",
    "\n",
    "- **TF = Term Frequency**  \n",
    "  > ‚ÄúHow often does this word appear in this document?‚Äù  \n",
    "  If a word appears many times in a document, it‚Äôs probably important *for that document*.\n",
    "\n",
    "- **IDF = Inverse Document Frequency**  \n",
    "  > ‚ÄúHow rare is this word across the whole collection of documents?‚Äù  \n",
    "  If a word appears in almost **every** document (like *‚Äúthe‚Äù* or *‚Äúis‚Äù*), it‚Äôs **less useful** for telling documents apart.  \n",
    "  If a word appears in only a **few** documents (like *‚ÄúRAG‚Äù* or *‚ÄúTF‚ÄëIDF‚Äù*), it‚Äôs **more informative**.\n",
    "\n",
    "**TF‚ÄëIDF weight = TF √ó IDF**  \n",
    "‚Üí high when a word is **frequent in this doc** but **rare overall**.\n",
    "\n",
    "Then we use **cosine similarity** between TF‚ÄëIDF vectors:\n",
    "\n",
    "> ‚ÄúHow close are these two vectors in angle?‚Äù  \n",
    "> Closer angle (cosine near 1.0) ‚Üí more similar content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40866be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>recall@3</th>\n",
       "      <th>precision@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  recall@3  precision@3\n",
       "0         0       0.5     0.333333\n",
       "1         1       1.0     0.333333\n",
       "2         2       1.0     0.333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a TF-IDF vectorizer on the documents\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "doc_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "def tfidf_retrieve(query: str, top_k: int = 5) -> List[int]:\n",
    "    \"\"\"Use TF-IDF + cosine similarity to rank documents for a query.\"\"\"\n",
    "    q_vec = tfidf_vectorizer.transform([query])\n",
    "    sims = cosine_similarity(q_vec, doc_tfidf)[0]  # shape: (num_docs,)\n",
    "    ranked_indices = np.argsort(-sims)  # sort descending\n",
    "    return ranked_indices[:top_k].tolist()\n",
    "\n",
    "\n",
    "# Evaluate TF-IDF retrieval\n",
    "results_tfidf = []\n",
    "\n",
    "for qi, q in enumerate(queries):\n",
    "    ranked_docs = tfidf_retrieve(q, top_k=5)\n",
    "    relevant = ground_truth_relevant[qi]\n",
    "    r_at_3 = recall_at_k(ranked_docs, relevant, k=3)\n",
    "    p_at_3 = precision_at_k(ranked_docs, relevant, k=3)\n",
    "    results_tfidf.append({\n",
    "        \"query_id\": qi,\n",
    "        \"query\": q,\n",
    "        \"ranked_docs\": ranked_docs,\n",
    "        \"recall@3\": r_at_3,\n",
    "        \"precision@3\": p_at_3,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results_tfidf)[[\"query_id\", \"recall@3\", \"precision@3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968d56a",
   "metadata": {},
   "source": [
    "### 4.1 Comparing Keyword vs TF‚ÄëIDF Retrieval\n",
    "\n",
    "Let‚Äôs put the metrics side by side to see if TF‚ÄëIDF helps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cee3156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>kw_recall@3</th>\n",
       "      <th>kw_precision@3</th>\n",
       "      <th>tfidf_recall@3</th>\n",
       "      <th>tfidf_precision@3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do RAG systems work?</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is TF-IDF in information retrieval?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do we evaluate search with precision and r...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      query  kw_recall@3   \n",
       "query_id                                                                   \n",
       "0                                  How do RAG systems work?          0.5  \\\n",
       "1                  What is TF-IDF in information retrieval?          1.0   \n",
       "2         How do we evaluate search with precision and r...          1.0   \n",
       "\n",
       "          kw_precision@3  tfidf_recall@3  tfidf_precision@3  \n",
       "query_id                                                     \n",
       "0               0.333333             0.5           0.333333  \n",
       "1               0.333333             1.0           0.333333  \n",
       "2               0.333333             1.0           0.333333  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kw = pd.DataFrame(results_keyword).set_index(\"query_id\")\n",
    "df_tf = pd.DataFrame(results_tfidf).set_index(\"query_id\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"query\": df_kw[\"query\"],\n",
    "    \"kw_recall@3\": df_kw[\"recall@3\"],\n",
    "    \"kw_precision@3\": df_kw[\"precision@3\"],\n",
    "    \"tfidf_recall@3\": df_tf[\"recall@3\"],\n",
    "    \"tfidf_precision@3\": df_tf[\"precision@3\"],\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2249f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAF2CAYAAAB9KhCBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMoklEQVR4nO3dd1QU198G8GdBWDqICCgiINiwgGJXrCh2jUbRmIgklhiNRtQIid0oNhCjxt5ixf4zFqyxE01QjEkURbFEBSUqYKPs3vcPXyZudkFAZB19PufsOe6dOzPfnR12H2fuzCqEEAJERERERDJkoO8CiIiIiIgKi2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthlkiPFAoFJk6cqO8yqJj169cPrq6u+i4DANC8eXM0b948X30fP34Me3t7rFu37s0WRZSHN/m5uWjRIpQvXx4ZGRlvZPn0ZjDMUrFbtWoVFAoFfvvtN4321NRU1KtXDyYmJoiOjtZTde8XV1dXKBSKVz5WrVoFALlOd3R0fOW6+vXrBwsLC4225s2bS8swMDCAlZUVKleujE8++QQHDhwocM3Pnz/Pdf3Xr1/X6GtgYABbW1u0a9cOMTEx+d9oL3n69CkmTpyII0eOFGp+uZk7dy4sLS3Rq1cvndO//vprKBQKBAQEFHNlVNR27dqFtm3bolSpUjAxMUGlSpUwevRoPHjwQN+lvVH9+vVDZmYmFi9erO9SqABK6LsAIgBIS0tDmzZt8Pvvv2P79u1o27atvkt6L0RGRuLx48fS8z179mDDhg2YM2cO7OzspPZGjRpJ/27dujX69u2rsRxTU9NC11CuXDmEhYUBAJ48eYKEhARs27YNa9euRc+ePbF27VoYGRlpzOPt7Y2RI0dqLcvY2PiV6+vduzfat28PlUqFy5cv44cffkCLFi3w66+/okaNGgWq/enTp5g0aRIA5PvoJgAsXboUarW6QOvSt6ysLMydOxcjRoyAoaGh1nQhBDZs2ABXV1f89NNPSE9Ph6WlpR4qpdc1atQohIeHw8vLC2PGjIGtrS3Onj2LefPmISoqCocOHULFihX1Vt+zZ89QosSbiS8mJiYIDAxEREQEvvzySygUijeyHipigqiYrVy5UgAQv/76qxBCiLS0NNGgQQNhbGwsdu3apefqitbjx4/znA5ATJgwoXiKyYdZs2YJACIxMVHndABiyJAhhVp2YGCgMDc312hr1qyZqFatmlbf7Oxs8cUXXwgA4uuvv9aY5uLiIjp06FDg9ScmJgoAYtasWRrte/fuFQDE4MGDC7zM+/fvF+g9fNX+oA/NmjUTzZo1e2W/bdu2CQAiISFB5/TDhw8LAOLw4cPCyMhIrFq1qogrfbWsrCyRkZFR7OuVE7VaLZ4+fZrr9PXr1wsAIiAgQGRnZ2tMO336tDAzMxNeXl4iKyvrTZeqQaVSiWfPnhXLun777TcBQBw6dKhY1kevj8MMSK8eP36Mtm3b4uzZs9i6dSs6dOigMf327dv49NNP4eDgAKVSiWrVqmHFihUa85ubm2P48OFay/77779haGiIsLAwPHr0CIaGhvj++++l6SkpKTAwMECpUqUghJDaBw8erHXafPPmzfDx8YGpqSns7Ozw8ccf4/bt2xp9ck6jX716Fe3bt4elpSX69OkDAMjIyMCIESNQunRpWFpaonPnzvj7779fuX2Sk5NRokQJ6ejfy+Lj46FQKDB//nwAL46cTZo0CRUrVoSJiQlKlSqFJk2a5Hq6/m2W8155enpi/vz5SE1NfWPr8vX1BQBcvXpVo/3Ro0f46quv4OzsDKVSCQ8PD8yYMUM6onr9+nWULl0aADBp0iRp+ELOWL689gddY2bVajUiIyNRrVo1mJiYwMHBAYMGDcLDhw+lPh07dkSFChV0vo6GDRuiTp060vOVK1eiZcuWsLe3h1KphKenJxYuXFjo7bRjxw64urrC3d1d5/R169bB09MTLVq0gJ+fn8a42oLsx8Crtz3w77CR2bNnIzIyEu7u7lAqlfjrr7+QmZmJ8ePHw8fHB9bW1jA3N4evry9+/vlnrfX/888/+OSTT2BlZQUbGxsEBgbi/PnzGsNrcly6dAkffvghbG1tYWJigjp16mDnzp352n5PnjzByJEjpddUuXJlzJ49W+Ozp3r16mjRooXWvGq1Gk5OTvjwww812l61vwAvhuV07NgR+/btQ506dWBqaprnKfRJkyahZMmSWLJkidYR+Hr16mHMmDE4f/48tm3bprGOfv36aS1L13jsjIwMTJgwAR4eHlAqlXB2dsbXX3+tNUZVoVBg6NChWLduHapVqwalUikNP9M1ZvZV3xU55s2bh2rVqsHMzAwlS5ZEnTp1sH79eo0+Pj4+sLW1xf/+979ctxO9XRhmSW+ePHmCdu3a4ddff8XmzZvRsWNHjenJyclo0KABDh48iKFDh2Lu3Lnw8PDAZ599hsjISACAhYUFPvjgA0RFRUGlUmnMv2HDBggh0KdPH9jY2KB69eo4duyYNP3EiRNQKBR48OAB/vrrL6n9+PHjUsABXozx7dmzpxSMBwwYgG3btqFJkyZ49OiRxjqzs7Ph7+8Pe3t7zJ49G927dwcA9O/fH5GRkWjTpg2mT58OIyMjreCui4ODA5o1a4ZNmzZpTYuKioKhoSF69OgBAJg4cSImTZqEFi1aYP78+fj2229Rvnx5nD179pXrKYjnz58jJSVF4/EmLpYwNDRE79698fTpU5w4cUJjWlZWllYNT58+LdR6rl+/DgAoWbKk1Pb06VM0a9YMa9euRd++ffH999+jcePGCA0NRXBwMACgdOnSUjj84IMPsGbNGqxZswbdunWTlpPb/qDLoEGDMHr0aDRu3Bhz585FUFAQ1q1bB39/f2RlZQEAAgICkJiYiF9//VVj3hs3buCXX37RGMu6cOFCuLi44JtvvkF4eDicnZ3xxRdfYMGCBYXaTqdOnULt2rV1TsvIyMDWrVvRu3dvAC+Gchw+fBhJSUkACrYf52fbv2zlypWYN28eBg4ciPDwcNja2iItLQ3Lli1D8+bNMWPGDEycOBH379+Hv78/4uLipHnVajU6deqEDRs2IDAwEFOnTsXdu3cRGBiotZ4///wTDRo0wMWLFxESEoLw8HCYm5uja9eu2L59e57bTgiBzp07Y86cOWjbti0iIiJQuXJljB49WuM1BQQE4NixY9J2y3HixAncuXNH4/3Nz/6SIz4+Hr1790br1q0xd+5ceHt766zzypUriI+PR5cuXWBlZaWzT84Qo59++inP16yLWq1G586dMXv2bHTq1Anz5s1D165dMWfOHJ3jrA8fPowRI0YgICAAc+fOzfWiyfx8VwAvhvcMGzYMnp6eiIyMxKRJk+Dt7Y3Tp09rLbN27do4efJkgV8j6Yl+DwzT+yhnmIGLi4swMjISO3bs0Nnvs88+E2XKlBEpKSka7b169RLW1tbSqbJ9+/YJAGLv3r0a/WrWrKlx+nTIkCHCwcFBeh4cHCyaNm0q7O3txcKFC4UQQvzzzz9CoVCIuXPnCiGEyMzMFPb29qJ69eoap7h27dolAIjx48dLbYGBgQKACAkJ0agjLi5OABBffPGFRvtHH32Ur1PUixcvFgDEhQsXNNo9PT1Fy5YtpedeXl6FOv3+svwMM9D1WLly5SuXXZBhBjm2b98uAEjvhxAvhhnoquFV2zFnmMGkSZPE/fv3RVJSkjh+/LioW7euACA2b94s9Z0yZYowNzcXly9f1lhGSEiIMDQ0FDdv3hRC5D3MILf9IWeai4uL9Pz48eMCgFi3bp1Gv+joaI321NRUoVQqxciRIzX6zZw5UygUCnHjxg2pTdepZH9/f1GhQgWNtvwMM8jKyhIKhUJrvTm2bNkiAIgrV64IIV4MHTIxMRFz5syR+uR3P87vts95P62srMS9e/c0+mZnZ2sNN3j48KFwcHAQn376qdS2detWAUBERkZKbSqVSrRs2VJrv27VqpWoUaOGeP78udSmVqtFo0aNRMWKFXVulxw7duwQAMR3332n0f7hhx8KhUIhDd2Ij48XAMS8efM0+n3xxRfCwsJCek/zu78I8e/fS3R0dJ41vlzny++bLlZWVqJ27doa6wgMDNTq9999a82aNcLAwEAcP35co9+iRYsEAHHy5EmpDYAwMDAQf/75p9Zy//s3l9/vii5duuT5efOygQMHClNT03z1Jf3jkVnSm+TkZJiYmMDZ2VlrmhACW7duRadOnSCE0DgC5+/vj9TUVOmIo5+fH8qWLatxWvOPP/7A77//jo8//lhq8/X1RXJyMuLj4wG8OALbtGlT+Pr64vjx4wBeHAERQkhHZn/77Tfcu3cPX3zxBUxMTKRldejQAVWqVMHu3bu1ah88eLDG8z179gAAhg0bptH+1Vdf5Ws7devWDSVKlEBUVJTG6/vrr780jmbY2Njgzz//xJUrV/K13MLq0qULDhw4oPHw9/d/I+vKuftBenq6Rnv9+vW1avjvRWm5mTBhAkqXLg1HR0f4+vri4sWLCA8P1ziFu3nzZvj6+qJkyZIa+56fnx9UKpXGEf5X+e/+oMvmzZthbW2N1q1ba6zPx8cHFhYW0ulxKysrtGvXDps2bdI4PR0VFYUGDRqgfPnyUtvLF+WlpqYiJSUFzZo1w7Vr1wo8bOPBgwcQQmgcvX7ZunXrUKdOHXh4eAAALC0t0aFDB42/yfzuxwXd9t27d5eGe+QwNDSULgZUq9V48OABsrOzUadOHY0zFdHR0TAyMsKAAQOkNgMDAwwZMkTr9R8+fBg9e/ZEenq6VNM///wDf39/XLlyRWvY0cv27NkDQ0NDrc+AkSNHQgiBvXv3AgAqVaoEb29vjW2kUqmwZcsWdOrUSXpP87u/5HBzc8vX32jO39mrLtyztLTU+pvMj82bN6Nq1aqoUqWKRt0tW7YEAK26mzVrBk9PzzyXWZDvChsbG/z9999aZzZ0KVmyJJ49e1boMz5UvHg3A9KbxYsXIzg4GG3btsXx48dRuXJladr9+/fx6NEjLFmyBEuWLNE5/7179wC8+PLp06cPFi5ciKdPn8LMzAzr1q2DiYmJdOoS+Hds5PHjx1GuXDmcO3cO3333HUqXLo3Zs2dL06ysrODl5QXgxelbABq15ahSpYrW6e8SJUqgXLlyGm03btyAgYGB1lhDXcvUxc7ODq1atcKmTZswZcoUAC/CS4kSJTROaU+ePBldunRBpUqVUL16dbRt2xaffPIJatasma/15Fe5cuXg5+enc9qzZ8+0glJ+btuVm5w7Lfz3y9XOzi7XGl5l4MCB6NGjB54/f47Dhw/j+++/1xqicuXKFfz+++9aISlHzr73Krr2B12uXLmC1NRU2Nvbv3J9AQEB2LFjB2JiYtCoUSNcvXoVsbGxGqdTAeDkyZOYMGECYmJitL6QU1NTYW1tna/X8LKXA3SOR48eYc+ePRg6dCgSEhKk9saNG2Pr1q24fPkyKlWqlO/9uKDb3s3NTWe/1atXIzw8HJcuXdI47f5y/xs3bqBMmTIwMzPTmDcnlOdISEiAEALjxo3DuHHjcq3LyclJ57QbN26gbNmyWvtx1apVpek5AgIC8M033+D27dtwcnLCkSNHcO/ePY3AX5D95b+vOS859b0qqKanpxfqPslXrlzBxYsXX/u9fVlBvivGjBmDgwcPol69evDw8ECbNm3w0UcfoXHjxlrz5OzrvJuBPDDMkt54enpiz549aNWqFVq3bo2TJ09KR2lzLvT4+OOPdY5fA6AR0vr27YtZs2Zhx44d6N27N9avX4+OHTtqfGGXLVsWbm5uOHbsGFxdXSGEQMOGDVG6dGkMHz4cN27cwPHjx9GoUSMYGBTupIVSqSz0vHnp1asXgoKCEBcXB29vb2zatAmtWrXSuH1W06ZNcfXqVfzvf//D/v37sWzZMsyZMweLFi1C//79i7wmXaKiohAUFKTRpisA5dcff/wBQDtcvI6KFStKQbhjx44wNDRESEgIWrRoIV1ApVar0bp1a3z99dc6l1GpUqV8rSu/+4Narc7zxwhe/vLv1KkTzMzMsGnTJjRq1AibNm2CgYGBxn/crl69ilatWqFKlSqIiIiAs7MzjI2NsWfPHsyZM6fAtwWztbWFQqHQurgIeHG0LSMjA+Hh4QgPD9eavm7dOunCr/zsxwXd9rpuC7d27Vr069cPXbt2xejRo2Fvby+Nef/vhX75kbO9Ro0alesRzqLaRwMCAhAaGorNmzfjq6++wqZNm2Btba1xu8KC7C9A/m+dl3MU9Pfff8+1z40bN5CWlqZxIWJugU+lUmlcRKZWq1GjRg1ERETo7P/fs3T5qbsg3xVVq1ZFfHw8du3ahejoaGzduhU//PADxo8fr3Vx4sOHD2FmZvZatx2k4sMwS3pVr1497NixAx06dEDr1q1x/PhxlC5dWrrqX6VS5esIXPXq1VGrVi2sW7cO5cqVw82bNzFv3jytfr6+vjh27Bjc3Nzg7e0NS0tLeHl5wdraGtHR0Th79qzGh5qLiwuAFxdQ5JwKyxEfHy9Nz4uLiwvUajWuXr2qcTQ2Z7hDfnTt2hWDBg2STj9evnwZoaGhWv1sbW0RFBSEoKAgPH78GE2bNsXEiROLLcz6+/sX2d0TVCoV1q9fDzMzMzRp0qRIlqnLt99+i6VLl2Ls2LHS1dLu7u54/PjxK/e9ojpq4+7ujoMHD6Jx48av/PI0NzdHx44dsXnzZkRERCAqKgq+vr4oW7as1Oenn35CRkYGdu7cqTH0QNfV/PlRokQJuLu7IzExUWvaunXrUL16dUyYMEFr2uLFi7F+/Xrpbyo/+3F+t31etmzZggoVKmDbtm0a79F/a3RxccHPP/8sndHJ8fIRZgBScDMyMipUXS4uLjh48KDWvXcvXbokTc/h5uaGevXqISoqCkOHDsW2bdvQtWtXKJVKqU9B9peCqFixIipXrowdO3ZIP5DxXz/++CMAaPznqWTJkloXwwIvgu/Lodfd3R3nz59Hq1atiuxvp6DfFebm5ggICEBAQAAyMzPRrVs3TJ06FaGhoRpDyRITE6Uj5/T245hZ0rtWrVphw4YNSEhIQNu2bZGWlgZDQ0N0794dW7dulY7Ovez+/ftabZ988gn279+PyMhIlCpVCu3atdPq4+vri+vXr0sBAHgxTKFRo0aIiIhAVlaWxp0M6tSpA3t7eyxatEjjiv29e/fi4sWL+bojQU4dL98WDIDWaeG82NjYwN/fH5s2bcLGjRthbGyMrl27avT5559/NJ5bWFjAw8OjWH+WsUyZMvDz89N4FIZKpcKwYcNw8eJFDBs2LNcrq4uCjY0NBg0ahH379klXuvfs2RMxMTHYt2+fVv9Hjx4hOzsbAKQApOuLvCB69uwJlUolnX5/WXZ2ttbyAwICcOfOHSxbtgznz5/XuhI852jYy0fFU1NTsXLlykLX2LBhQ61f7bt16xaOHTuGnj174sMPP9R6BAUFISEhQbpaPD/7cX63fV50vf7Tp09r/dJbzpX/S5culdrUarXWHR/s7e3RvHlzLF68GHfv3tVan67Po5fl/EjHy7cfA4A5c+ZAoVBofVYFBATgl19+wYoVK5CSkqL1/hZ0fymICRMm4OHDh/j888+1ht/ExsZixowZqFWrlkbN7u7u+OWXX5CZmSm17dq1C7du3dKq+/bt2xrbO8ezZ8/w5MmTAtdbkO+K/35GGhsbw9PTE0IIrTtAnD17VuPHYugtV/zXnNH77r8/mpBjxYoVAoBo1qyZePbsmUhKShIuLi7CzMxMDB8+XCxevFiEhYWJHj16iJIlS2otNykpSZQoUSLPG+BfunRJuvp969atUntYWJgAIJRKpcbVyi/XW79+fREZGSlCQ0OFmZmZcHV1FQ8fPpT66bpaP0fv3r0FANGnTx+xYMEC0a1bN1GzZs0C3XB/7dq1AoCwtLQUnTp10ppub28vevbsKWbMmCGWLl0qBg0aJBQKhfjyyy/ztXwh9POjCeXKlRNr1qwRa9asEYsXLxajR48W7u7uAoDo1auX1s3Zi/pHE4QQ4vbt28LY2FgEBAQIIYR48uSJqF27tihRooTo37+/WLhwoZg9e7b0Gu7fvy/N6+npKRwdHcWCBQvEhg0bpKv189of/ns3AyGEGDRokAAg2rVrJ+bMmSPmz58vhg8fLsqWLatxpwUhhHj27JmwtLQUlpaWwtDQUCQnJ2tMv3TpkjA2NhY1atQQ8+fPF9OnTxfu7u7Cy8tL6/3N748m5NyxID4+XmqbPn26ACDi4uJ0zvPw4UNRokQJjX3wVftxfrd9Xu9nzmdJ586dxeLFi0VISIiwsbER1apV09ju2dnZol69esLQ0FAMHTpUzJ8/X7Rp00Z4e3sLABo//PDnn3+KkiVLilKlSomQkBCxZMkSMWXKFNG+fXtRs2bNPLedSqUSLVq0EAqFQgwcOFAsWLBAdOnSRQAQX331lVb/W7duCYVCISwtLYWtra3IzMzU6pPf/aUwfy/BwcECgPD29hazZs0Sy5YtE1988YUwMTERzs7O4urVqxr9c+6i0KJFC7Fw4UIxatQo4ejoKNzd3TX2LZVKJdq3by8UCoXo1auXmDdvnoiMjBSff/65sLW11fhOyOuz5r+fm/n9rqhdu7Zo3769mDp1qli2bJkYOXKkUCqVWvthzo8mHDx4sEDbjfSHYZaKXW5hVgghZs+eLQCIjh07iqysLJGcnCyGDBkinJ2dhZGRkXB0dBStWrUSS5Ys0bns9u3bCwDi1KlTua7f3t5eANAIACdOnBAAhK+vr855oqKiRK1atYRSqRS2traiT58+4u+//9bok1d4efbsmRg2bJgoVaqUMDc3F506dRK3bt0qUJhNS0sTpqamAoBYu3at1vTvvvtO1KtXT9jY2AhTU1NRpUoVMXXqVJ1fhLnRR5jN+c8FAGFhYSEqVqwoPv74Y7F//36dy3kTYVYIIfr16ycMDQ2l2ySlp6eL0NBQ4eHhIYyNjYWdnZ1o1KiRmD17tsY2PXXqlPDx8RHGxsYa72dBw6wQQixZskT4+PgIU1NTYWlpKWrUqCG+/vprcefOHa2+ffr0EQCEn5+fznXs3LlT1KxZU5iYmAhXV1cxY8YMKeQVJsxmZGQIOzs7MWXKFKmtRo0aonz58nnO17x5c2Fvby/9p+RV+7EQ+dv2eb2farVaTJs2Tbi4uAilUilq1aoldu3apXO7379/X3z00UfC0tJSWFtbi379+omTJ08KAGLjxo0afa9evSr69u0rHB0dhZGRkXBychIdO3YUW7ZseeX2S09PFyNGjBBly5YVRkZGomLFimLWrFlCrVbr7N+4cWMBQPTv3z/XZeZnfyns38vOnTuFn5+fsLGxkf4+q1WrJlJTU3X2Dw8PF05OTkKpVIrGjRuL3377Tee+lZmZKWbMmCGqVasmlEqlKFmypPDx8RGTJk3SWHZBwqwQIl/fFYsXLxZNmzYVpUqVEkqlUri7u4vRo0drvaYxY8aI8uXL5/re0NtHIcRrXJ1B9Jb54IMPcOHCBa0xb0T0+qZMmYKVK1fiypUrWr8O9S7ZsWMHPvjgA5w4cULnle7vo/79+2P58uVYunRpsY3B14eMjAy4uroiJCRE5y9L0tuJY2bpnXH37l3s3r0bn3zyib5LIXonjRgxAo8fP8bGjRv1XUqRefbsmcZzlUqFefPmwcrKKtdfPHsfLV68GB07dsTgwYOle2e/i1auXAkjIyN8/vnn+i6FCoBHZkn2EhMTcfLkSSxbtgy//vorrl69+lr3NiWi90f//v3x7NkzNGzYEBkZGdi2bRtOnTqFadOm6bxjCBG9fXhrLpK9o0ePIigoCOXLl8fq1asZZIko31q2bInw8HDs2rULz58/h4eHB+bNm4ehQ4fquzQiyicemSUiIiIi2eKYWSIiIiKSLYZZIiIiIpKt927MrFqtxp07d2BpaVlkP6dHREREREVHCIH09HSULVsWBgZ5H3t978LsnTt34OzsrO8yiIiIiOgVbt26hXLlyuXZ570Ls5aWlgBebJw3+XvvRERERFQ4aWlpcHZ2lnJbXt67MJsztMDKyophloiIiOgtlp8hobwAjIiIiIhki2GWiIiIiGSLYZaIiIiIZOu9GzNLRERE7zaVSoWsrCx9l0GvYGxs/MrbbuUHwywRERG9E4QQSEpKwqNHj/RdCuWDgYEB3NzcYGxs/FrLYZglIiKid0JOkLW3t4eZmRl/HOktlvMjVnfv3kX58uVf671imCUiIiLZU6lUUpAtVaqUvsuhfChdujTu3LmD7OxsGBkZFXo5er0A7NixY+jUqRPKli0LhUKBHTt2vHKeI0eOoHbt2lAqlfDw8MCqVaveeJ1ERET0dssZI2tmZqbnSii/coYXqFSq11qOXsPskydP4OXlhQULFuSrf2JiIjp06IAWLVogLi4OX331Ffr37499+/a94UqJiIhIDji0QD6K6r3S6zCDdu3aoV27dvnuv2jRIri5uSE8PBwAULVqVZw4cQJz5syBv7//myqTiIiIiN5SsrrPbExMDPz8/DTa/P39ERMTo6eKiIiIiEiXVatWwcbG5o2vR1YXgCUlJcHBwUGjzcHBAWlpaXj27BlMTU215snIyEBGRob0PC0t7Y3XSURERG8P15Ddxbau69M7FHiefv364dGjRxrXDm3ZsgUff/wxpk6dipEjRxZhhe8eWR2ZLYywsDBYW1tLD2dnZ32XRERERJSrZcuWoU+fPli4cKEsgmxmZqZe1y+rMOvo6Ijk5GSNtuTkZFhZWek8KgsAoaGhSE1NlR63bt0qjlKJiIiICmzmzJn48ssvsXHjRgQFBQEA/ve//6F27dowMTFBhQoVMGnSJGRnZwMAPv30U3Ts2FFjGVlZWbC3t8fy5cuxa9cu2NjYSHcMiIuLg0KhQEhIiNS/f//++Pjjj6XnW7duRbVq1aBUKuHq6ipdq5TD1dUVU6ZMQd++fWFlZYWBAwcCeDGsoHz58jAzM8MHH3yAf/75p+g3kA6yCrMNGzbEoUOHNNoOHDiAhg0b5jqPUqmElZWVxoOIiIjobTNmzBhMmTIFu3btwgcffAAAOH78OPr27Yvhw4fjr7/+wuLFi7Fq1SpMnToVwIsgGh0djbt370rL2bVrF54+fYqAgAD4+voiPT0d586dAwAcPXoUdnZ2OHLkiNT/6NGjaN68OQAgNjYWPXv2RK9evXDhwgVMnDgR48aN07oV6uzZs+Hl5YVz585h3LhxOH36ND777DMMHToUcXFxaNGiBb777rs3t7Feotcw+/jxY8TFxSEuLg7Ai1tvxcXF4ebNmwBeHFXt27ev1P/zzz/HtWvX8PXXX+PSpUv44YcfsGnTJowYMUIf5RMREREVib1792LmzJn43//+h1atWkntkyZNQkhICAIDA1GhQgW0bt0aU6ZMweLFiwEAjRo1QuXKlbFmzRppnpUrV6JHjx6wsLCAtbU1vL29pfB65MgRjBgxAufOncPjx49x+/ZtJCQkoFmzZgCAiIgItGrVCuPGjUOlSpXQr18/DB06FLNmzdKot2XLlhg5ciTc3d3h7u6OuXPnom3btvj6669RqVIlDBs2rNjuNKXXMPvbb7+hVq1aqFWrFgAgODgYtWrVwvjx4wEAd+/elYItALi5uWH37t04cOAAvLy8EB4ejmXLlvG2XERERCRrNWvWhKurKyZMmIDHjx9L7efPn8fkyZNhYWEhPQYMGIC7d+/i6dOnAF4cnV25ciWAF8Mv9+7di08//VRaRrNmzXDkyBEIIXD8+HF069ZNur3p0aNHUbZsWVSsWBEAcPHiRTRu3FijtsaNG+PKlSsaP25Qp04djT4XL15E/fr1NdryOnNelPR6N4PmzZtDCJHrdF2/7tW8eXPpUDkRERHRu8DJyQlbtmxBixYt0LZtW+zduxeWlpZ4/PgxJk2ahG7dumnNY2JiAgDo27cvQkJCEBMTg1OnTsHNzQ2+vr5Sv+bNm2PFihU4f/48jIyMUKVKFTRv3hxHjhzBw4cPpaOyBWFubl74F1vEZHVrLiIiIqJ3lYuLC44ePSoF2ujoaNSuXRvx8fHw8PDIdb5SpUqha9euWLlyJWJiYqQLx3LkjJudM2eOFFybN2+O6dOn4+HDhxp3TKhatSpOnjypMf/JkydRqVIlGBoa5lpD1apVcfr0aY22X375Jd+v/XUwzBIRERG9JZydnXHkyBG0aNEC/v7+GDNmDD788EOUL18eH374IQwMDHD+/Hn88ccfGhdY9e/fHx07doRKpUJgYKDGMkuWLImaNWti3bp1mD9/PgCgadOm6NmzJ7KysjSOzI4cORJ169bFlClTEBAQgJiYGMyfPx8//PBDnnUPGzYMjRs3xuzZs9GlSxfs27cP0dHRRbhlcieruxkQERERvevKlSuHI0eOICUlBdOnT8eWLVuwf/9+1K1bFw0aNMCcOXPg4uKiMY+fnx/KlCkDf39/lC1bVmuZzZo1g0qlku5aYGtrC09PTzg6OqJy5cpSv9q1a2PTpk3YuHEjqlevjvHjx2Py5Mno169fnjU3aNAAS5cuxdy5c+Hl5YX9+/dj7Nixr70t8kMh8hq0+g5KS0uDtbU1UlNTeZsuIiKid8Tz58+RmJgINzc3aSzp++Tx48dwcnLCypUrdY6vfRvl9Z4VJK9xmAERERGRTKnVaqSkpCA8PBw2Njbo3LmzvksqdgyzRERERDJ18+ZNuLm5oVy5cli1ahVKlHj/ot3794qJiIiI3hGurq553ub0fcALwIiIiIhIthhmiYiIiEi2GGaJiIiISLYYZomIiIhIthhmiYiIiEi2GGaJiIiISLYYZomIiIhItnifWSIiInq3TbQuxnWlFqi7QqHIc/qECRPQr18/uLm5aU3r06cP1q5dq3O+I0eOoEWLFnj48CFsbGyk5znrtLS0RIUKFdC6dWuMGDECZcqU+fclTJyISZMmaS3zwIED8PPzK8jLKxYMs0RERER6cvfuXenfUVFRGD9+POLj46U2CwsLpKSkAAAOHjyIatWqSdNMTU0LvL74+HhYWVkhLS0NZ8+excyZM7F8+XIcOXIENWrUkPpVq1YNBw8e1JjX1ta2wOsrDgyzRERERHri6Ogo/dva2hoKhUKjDYAUZkuVKqU1raDs7e1hY2MDR0dHVKpUCV26dEGtWrUwePBgnDhxQupXokSJ115XceGYWSIiIqL3lKmpKT7//HOcPHkS9+7d03c5hcIwS0RERCQDjRo1goWFhfQ4d+5ckSy3SpUqAIDr169LbRcuXNBYV7169YpkXW8ChxkQERERyUBUVBSqVq0qPXd2dgbwYnzrjRs3AAC+vr7Yu3dvgZYrhACgeTFa5cqVsXPnTum5UqksdN1vGsMsERERkQw4OzvDw8NDq33Pnj3IysoCULiLwi5evAgAcHV1ldqMjY11ruttxDBLREREJGMuLi6FnvfZs2dYsmQJmjZtitKlSxdhVcWHYZaIiIjoPXHv3j08f/4c6enpiI2NxcyZM5GSkoJt27bpu7RCY5glIiIiek9UrlwZCoUCFhYWqFChAtq0aYPg4GDZ3IZLF4XIGfX7nkhLS4O1tTVSU1NhZWWl73KIiIioCDx//hyJiYlwc3ODiYmJvsuhfMjrPStIXuOtuYiIiIhIthhmiYiIiEi2GGaJiIiISLYYZomIiIhIthhmiYiIiEi2GGaJiIjonaFWq/VdAuVTUd1Qi/eZJSIiItkzNjaGgYEB7ty5g9KlS8PY2BgKhULfZVEuhBC4f/8+FAoFjIyMXmtZDLNEREQkewYGBnBzc8Pdu3dx584dfZdD+aBQKFCuXDkYGhq+1nIYZomIiOidYGxsjPLlyyM7OxsqlUrf5dArGBkZvXaQBRhmiYiI6B2Sc9r6dU9dk3zwAjAiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLf5oQjFwDdmt7xKoiF03+UjfJVBRmpiq7wqIiKiQeGSWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGRL72F2wYIFcHV1hYmJCerXr48zZ87k2T8yMhKVK1eGqakpnJ2dMWLECDx//ryYqiUiIiKit4lew2xUVBSCg4MxYcIEnD17Fl5eXvD398e9e/d09l+/fj1CQkIwYcIEXLx4EcuXL0dUVBS++eabYq6ciIiIiN4Geg2zERERGDBgAIKCguDp6YlFixbBzMwMK1as0Nn/1KlTaNy4MT766CO4urqiTZs26N279yuP5hIRERHRu0lvYTYzMxOxsbHw8/P7txgDA/j5+SEmJkbnPI0aNUJsbKwUXq9du4Y9e/agffv2xVIzEREREb1d9PajCSkpKVCpVHBwcNBod3BwwKVLl3TO89FHHyElJQVNmjSBEALZ2dn4/PPP8xxmkJGRgYyMDOl5Wlpa0bwAIiIiItI7vV8AVhBHjhzBtGnT8MMPP+Ds2bPYtm0bdu/ejSlTpuQ6T1hYGKytraWHs7NzMVZMRERERG+S3o7M2tnZwdDQEMnJyRrtycnJcHR01DnPuHHj8Mknn6B///4AgBo1auDJkycYOHAgvv32WxgYaGfz0NBQBAcHS8/T0tIYaImIiIjeEXo7MmtsbAwfHx8cOnRIalOr1Th06BAaNmyoc56nT59qBVZDQ0MAgBBC5zxKpRJWVlYaDyIiIiJ6N+jtyCwABAcHIzAwEHXq1EG9evUQGRmJJ0+eICgoCADQt29fODk5ISwsDADQqVMnREREoFatWqhfvz4SEhIwbtw4dOrUSQq1RERERPT+0GuYDQgIwP379zF+/HgkJSXB29sb0dHR0kVhN2/e1DgSO3bsWCgUCowdOxa3b99G6dKl0alTJ0ydOlVfL4GIiIiI9Eghcjs//45KS0uDtbU1UlNTi23IgWvI7mJZDxWf6yYf6bsEKkoTU/VdARERvaQgeU1WdzMgIiIiInoZwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJlt7D7IIFC+Dq6goTExPUr18fZ86cybP/o0ePMGTIEJQpUwZKpRKVKlXCnj17iqlaIiIiInqblNDnyqOiohAcHIxFixahfv36iIyMhL+/P+Lj42Fvb6/VPzMzE61bt4a9vT22bNkCJycn3LhxAzY2NsVfPBERERHpnV7DbEREBAYMGICgoCAAwKJFi7B7926sWLECISEhWv1XrFiBBw8e4NSpUzAyMgIAuLq6FmfJRERERPQW0dswg8zMTMTGxsLPz+/fYgwM4Ofnh5iYGJ3z7Ny5Ew0bNsSQIUPg4OCA6tWrY9q0aVCpVLmuJyMjA2lpaRoPIiIiIno36C3MpqSkQKVSwcHBQaPdwcEBSUlJOue5du0atmzZApVKhT179mDcuHEIDw/Hd999l+t6wsLCYG1tLT2cnZ2L9HUQERERkf7o/QKwglCr1bC3t8eSJUvg4+ODgIAAfPvtt1i0aFGu84SGhiI1NVV63Lp1qxgrJiIiIqI3SW9jZu3s7GBoaIjk5GSN9uTkZDg6Ouqcp0yZMjAyMoKhoaHUVrVqVSQlJSEzMxPGxsZa8yiVSiiVyqItnoiIiIjeCno7MmtsbAwfHx8cOnRIalOr1Th06BAaNmyoc57GjRsjISEBarVaart8+TLKlCmjM8gSERER0btNr8MMgoODsXTpUqxevRoXL17E4MGD8eTJE+nuBn379kVoaKjUf/DgwXjw4AGGDx+Oy5cvY/fu3Zg2bRqGDBmir5dARERERHr0WsMMMjMzkZiYCHd3d5QoUfBFBQQE4P79+xg/fjySkpLg7e2N6Oho6aKwmzdvwsDg37zt7OyMffv2YcSIEahZsyacnJwwfPhwjBkz5nVeBhERERHJlEIIIQo609OnT/Hll19i9erVAF6c6q9QoQK+/PJLODk56bxH7NsiLS0N1tbWSE1NhZWVVbGs0zVkd7Gsh4rPdZOP9F0CFaWJqfqugIiIXlKQvFaoYQahoaE4f/48jhw5AhMTE6ndz88PUVFRhVkkEREREVGBFWqYwY4dOxAVFYUGDRpAoVBI7dWqVcPVq1eLrDgiIiIiorwU6sjs/fv3YW9vr9X+5MkTjXBLRERERPQmFSrM1qlTB7t3/zsONCfALlu2LNfbahERERERFbVCDTOYNm0a2rVrh7/++gvZ2dmYO3cu/vrrL5w6dQpHjx4t6hqJiIiIiHQq1JHZJk2a4Pz588jOzkaNGjWwf/9+2NvbIyYmBj4+PkVdIxERERGRTgU+MpuVlYVBgwZh3LhxWLp06ZuoiYiIiIgoXwp8ZNbIyAhbt259E7UQERERERVIoYYZdO3aFTt27CjiUoiIiIiICqZQF4BVrFgRkydPxsmTJ+Hj4wNzc3ON6cOGDSuS4oiIiIiI8lKoMLt8+XLY2NggNjYWsbGxGtMUCgXDLBEREREVi0KF2cTExKKug4iIiIiowAo1ZvZlQggIIYqiFiIiIiKiAil0mP3xxx9Ro0YNmJqawtTUFDVr1sSaNWuKsjYiIiIiojwVaphBREQExo0bh6FDh6Jx48YAgBMnTuDzzz9HSkoKRowYUaRFEhERERHpUqgwO2/ePCxcuBB9+/aV2jp37oxq1aph4sSJDLNEREREVCwKNczg7t27aNSokVZ7o0aNcPfu3dcuioiIiIgoPwoVZj08PLBp0yat9qioKFSsWPG1iyIiIiIiyo9CDTOYNGkSAgICcOzYMWnM7MmTJ3Ho0CGdIZeIiIiI6E0o1JHZ7t274/Tp07Czs8OOHTuwY8cO2NnZ4cyZM/jggw+KukYiIiIiIp0KdWQWAHx8fLB27dqirIWIiIiIqEAKdWR2z5492Ldvn1b7vn37sHfv3tcuioiIiIgoPwoVZkNCQqBSqbTahRAICQl57aKIiIiIiPKjUGH2ypUr8PT01GqvUqUKEhISXrsoIiIiIqL8KFSYtba2xrVr17TaExISYG5u/tpFERERERHlR6HCbJcuXfDVV1/h6tWrUltCQgJGjhyJzp07F1lxRERERER5KVSYnTlzJszNzVGlShW4ubnBzc0NVapUQalSpTB79uyirpGIiIiISKdC3ZrL2toap06dwoEDB3D+/HmYmprCy8sLvr6+RV0fEREREVGuCnRkNiYmBrt27QIAKBQKtGnTBvb29pg9eza6d++OgQMHIiMj440USkRERET0XwUKs5MnT8aff/4pPb9w4QIGDBiA1q1bIyQkBD/99BPCwsKKvEgiIiIiIl0KFGbj4uLQqlUr6fnGjRtRr149LF26FMHBwfj++++xadOmIi+SiIiIiEiXAoXZhw8fwsHBQXp+9OhRtGvXTnpet25d3Lp1q+iqIyIiIiLKQ4HCrIODAxITEwEAmZmZOHv2LBo0aCBNT09Ph5GRUdFWSERERESUiwKF2fbt2yMkJATHjx9HaGgozMzMNO5g8Pvvv8Pd3b3IiyQiIiIi0qVAt+aaMmUKunXrhmbNmsHCwgKrV6+GsbGxNH3FihVo06ZNkRdJRERERKRLgcKsnZ0djh07htTUVFhYWMDQ0FBj+ubNm2FhYVGkBRIRERER5abQP5qgi62t7WsVQ0RERERUEIX6OVsiIiIiorcBwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREckWwywRERERyRbDLBERERHJFsMsEREREcnWWxFmFyxYAFdXV5iYmKB+/fo4c+ZMvubbuHEjFAoFunbt+mYLJCIiIqK3kt7DbFRUFIKDgzFhwgScPXsWXl5e8Pf3x7179/Kc7/r16xg1ahR8fX2LqVIiIiIietvoPcxGRERgwIABCAoKgqenJxYtWgQzMzOsWLEi13lUKhX69OmDSZMmoUKFCsVYLRERERG9TfQaZjMzMxEbGws/Pz+pzcDAAH5+foiJicl1vsmTJ8Pe3h6fffZZcZRJRERERG+pEvpceUpKClQqFRwcHDTaHRwccOnSJZ3znDhxAsuXL0dcXFy+1pGRkYGMjAzpeVpaWqHrJSIiIqK3i96HGRREeno6PvnkEyxduhR2dnb5micsLAzW1tbSw9nZ+Q1XSURERETFRa9HZu3s7GBoaIjk5GSN9uTkZDg6Omr1v3r1Kq5fv45OnTpJbWq1GgBQokQJxMfHw93dXWOe0NBQBAcHS8/T0tIYaImIiIjeEXoNs8bGxvDx8cGhQ4ek22up1WocOnQIQ4cO1epfpUoVXLhwQaNt7NixSE9Px9y5c3WGVKVSCaVS+UbqJyIiIiL90muYBYDg4GAEBgaiTp06qFevHiIjI/HkyRMEBQUBAPr27QsnJyeEhYXBxMQE1atX15jfxsYGALTaiYiIiOjdp/cwGxAQgPv372P8+PFISkqCt7c3oqOjpYvCbt68CQMDWQ3tJSIiIqJiohBCCH0XUZzS0tJgbW2N1NRUWFlZFcs6XUN2F8t6qPhcN/lI3yVQUZqYqu8KiIjoJQXJazzkSURERESyxTBLRERERLLFMEtEREREssUwS0RERESyxTBLRERERLLFMEtEREREssUwS0RERESyxTBLRERERLLFMEtEREREssUwS0RERESyxTBLRERERLLFMEtEREREssUwS0RERESyxTBLRERERLJVQt8FEBER5XAN2a3vEqgIXTf5SN8lUFGamKrvCnTikVkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpIthlkiIiIiki2GWSIiIiKSLYZZIiIiIpKttyLMLliwAK6urjAxMUH9+vVx5syZXPsuXboUvr6+KFmyJEqWLAk/P788+xMRERHRu0vvYTYqKgrBwcGYMGECzp49Cy8vL/j7++PevXs6+x85cgS9e/fGzz//jJiYGDg7O6NNmza4fft2MVdORERERPqm9zAbERGBAQMGICgoCJ6enli0aBHMzMywYsUKnf3XrVuHL774At7e3qhSpQqWLVsGtVqNQ4cOFXPlRERERKRveg2zmZmZiI2NhZ+fn9RmYGAAPz8/xMTE5GsZT58+RVZWFmxtbd9UmURERET0liqhz5WnpKRApVLBwcFBo93BwQGXLl3K1zLGjBmDsmXLagTil2VkZCAjI0N6npaWVviCiYiIiOitovdhBq9j+vTp2LhxI7Zv3w4TExOdfcLCwmBtbS09nJ2di7lKIiIiInpT9Bpm7ezsYGhoiOTkZI325ORkODo65jnv7NmzMX36dOzfvx81a9bMtV9oaChSU1Olx61bt4qkdiIiIiLSP72GWWNjY/j4+GhcvJVzMVfDhg1znW/mzJmYMmUKoqOjUadOnTzXoVQqYWVlpfEgIiIioneDXsfMAkBwcDACAwNRp04d1KtXD5GRkXjy5AmCgoIAAH379oWTkxPCwsIAADNmzMD48eOxfv16uLq6IikpCQBgYWEBCwsLvb0OIiIiIip+eg+zAQEBuH//PsaPH4+kpCR4e3sjOjpauijs5s2bMDD49wDywoULkZmZiQ8//FBjORMmTMDEiROLs3QiIiIi0jO9h1kAGDp0KIYOHapz2pEjRzSeX79+/c0XRERERESyIOu7GRARERHR+41hloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGSLYZaIiIiIZIthloiIiIhki2GWiIiIiGTrrQizCxYsgKurK0xMTFC/fn2cOXMmz/6bN29GlSpVYGJigho1amDPnj3FVCkRERERvU30HmajoqIQHByMCRMm4OzZs/Dy8oK/vz/u3buns/+pU6fQu3dvfPbZZzh37hy6du2Krl274o8//ijmyomIiIhI3/QeZiMiIjBgwAAEBQXB09MTixYtgpmZGVasWKGz/9y5c9G2bVuMHj0aVatWxZQpU1C7dm3Mnz+/mCsnIiIiIn0roc+VZ2ZmIjY2FqGhoVKbgYEB/Pz8EBMTo3OemJgYBAcHa7T5+/tjx44dOvtnZGQgIyNDep6amgoASEtLe83q80+d8bTY1kXFI00h9F0CFaVi/DygvPHz8t3Cz8p3TDF+VubkNCFevQ/pNcympKRApVLBwcFBo93BwQGXLl3SOU9SUpLO/klJSTr7h4WFYdKkSVrtzs7OhayaCLDWdwFUtKbzHSV6E/iX9Y7Rw2dleno6rK3zXq9ew2xxCA0N1TiSq1ar8eDBA5QqVQoKhUKPlZFcpaWlwdnZGbdu3YKVlZW+yyEieivxs5JehxAC6enpKFu27Cv76jXM2tnZwdDQEMnJyRrtycnJcHR01DmPo6NjgforlUoolUqNNhsbm8IXTfT/rKys+AFNRPQK/KykwnrVEdkcer0AzNjYGD4+Pjh06JDUplarcejQITRs2FDnPA0bNtToDwAHDhzItT8RERERvbv0PswgODgYgYGBqFOnDurVq4fIyEg8efIEQUFBAIC+ffvCyckJYWFhAIDhw4ejWbNmCA8PR4cOHbBx40b89ttvWLJkiT5fBhERERHpgd7DbEBAAO7fv4/x48cjKSkJ3t7eiI6Oli7yunnzJgwM/j2A3KhRI6xfvx5jx47FN998g4oVK2LHjh2oXr26vl4CvWeUSiUmTJigNXyFiIj+xc9KKi4KkZ97HhARERERvYX0/qMJRERERESFxTBLRERERLLFMEtEREREssUwS/QaFAqF9FPK169fh0KhQFxcnF5rIiJ6U17+zCvKvkSvg2GWZKtfv35QKBRQKBQwMjKCm5sbvv76azx//lzfpWn566+/MHjwYFStWhWlSpVCxYoVERgYiJiYGK2+8fHxaNGiBRwcHGBiYoIKFSpg7NixyMrK0kPlRPS2evkz0NjYGB4eHpg8eTKys7Pf2Drv3r2Ldu3aFXnfgnr8+DHCw8PRpEkTODo6wsnJCS1btsTixYt1vv5BgwbB3d0dpqamKF26NLp06YJLly69kdqo+DHMkqy1bdsWd+/exbVr1zBnzhwsXrwYEyZM0HdZGqZPn4769etDrVZj9uzZOHr0KFauXIkKFSqgc+fOCA0N1ehvZGSEvn37Yv/+/YiPj0dkZCSWLl361r0uItK/nM/AK1euYOTIkZg4cSJmzZql1S8zM7NI1ufo6JjvW20VpG9BxMbGwtPTEzt27MCAAQOwc+dO7Nq1C4GBgVi1ahXq1q2Le/fuaczj4+ODlStX4uLFi9i3bx+EEGjTpg1UKlWR10d6IIhkKjAwUHTp0kWjrVu3bqJWrVpCCCFUKpWYNm2acHV1FSYmJqJmzZpi8+bNGv3/+OMP0aFDB2FpaSksLCxEkyZNREJCghBCiDNnzgg/Pz9RqlQpYWVlJZo2bSpiY2M15gcgtm/fLoQQIjExUQAQ586dk6bPnz9fuLu7i/j4eJ2v4d69e6JWrVpi9uzZeb7WESNGiCZNmrxqkxDRe0TXZ2Dr1q1FgwYNpGnfffedKFOmjHB1dRVCCHHz5k3Ro0cPYW1tLUqWLCk6d+4sEhMTNZaxfPly4enpKYyNjYWjo6MYMmSINO3lz7yMjAwxZMgQ4ejoKJRKpShfvryYNm2azr5CCPH777+LFi1aCBMTE2FraysGDBgg0tPTtV7PrFmzhKOjo7C1tRVffPGFyMzMlPpcv35d2NvbiyVLlujcJmq1WowbN07Url1bY77/On/+vAAgfd6TvPHILL0z/vjjD5w6dQrGxsYAgLCwMPz4449YtGgR/vzzT4wYMQIff/wxjh49CgC4ffs2mjZtCqVSicOHDyM2NhaffvqpdIoqPT0dgYGBOHHiBH755RdUrFgR7du3R3p6er7qSUlJwfjx47F9+3ZUqlQJ27dvR/Xq1VG2bFmMHTsWrVu3xqVLl7BhwwZMnTo11+UmJCQgOjoazZo1K4KtRETvMlNTU+ko7KFDhxAfH48DBw5g165dyMrKgr+/PywtLXH8+HGcPHkSFhYWaNu2rTTPwoULMWTIEAwcOBAXLlzAzp074eHhoXNd33//PXbu3IlNmzYhPj4e69atg6urq86+T548gb+/P0qWLIlff/0VmzdvxsGDBzF06FCNfj///DOuXr2Kn3/+GatXr8aqVauwatUqaXpISAiCgoIwYMAA/P333+jYsSPs7e3h7++PKVOmYPDgwZg8eTLMzc2xdu3aXGtZuXIl3Nzc4OzsXMAtTG8lfadposIKDAwUhoaGwtzcXCiVSgFAGBgYiC1btojnz58LMzMzcerUKY15PvvsM9G7d28hhBChoaHCzc0tz/+9v0ylUglLS0vx008/SW3I48jskiVLRPfu3YUQQiQkJAilUinmz58vzp07Jz777DNhaGgofv75ZyGEEE2aNBF79+7VWF/Dhg2l1zVw4EChUqkKuomI6B328pFZtVotDhw4IJRKpRg1apQIDAwUDg4OIiMjQ+q/Zs0aUblyZaFWq6W2jIwMYWpqKvbt2yeEEKJs2bLi22+/zXWdL3/mffnll6Jly5Yay8ut75IlS0TJkiXF48ePpem7d+8WBgYGIikpSXo9Li4uIjs7W+rTo0cPERAQIIQQIj09XVhaWoqUlBQhhBAtW7YUnTt3FrGxsWLt2rXCwsJCBAYGCiGEWLZsmTRfjgULFghzc3MBQFSuXJlHZd8hPDJLstaiRQvExcXh9OnTCAwMRFBQELp3746EhAQ8ffoUrVu3hoWFhfT48ccfcfXqVQBAXFwcfH19YWRkpHPZycnJGDBgACpWrAhra2tYWVnh8ePHuHnzZr5qu3DhAho1agQA2LdvH5o2bYohQ4bA29sbP/zwg8ZYsjJlyuDhw4ca80dFReHs2bNYv349du/ejdmzZxdmExHRO2zXrl2wsLCAiYkJ2rVrh4CAAEycOBEAUKNGDelMFQCcP38eCQkJsLS0lD4TbW1t8fz5c1y9ehX37t3DnTt30KpVq3ytu1+/foiLi0PlypUxbNgw7N+/P9e+Fy9ehJeXF8zNzaW2xo0bQ61WIz4+XmqrVq0aDA0NpedlypSRxr9evnwZrq6uKFWqFJ48eYLDhw9j4cKFqF27Nvr06YNevXppzPffz9Q+ffrg3LlzOHr0KCpVqoSePXu+lRcMU8GV0HcBRK/D3NxcOgW2YsUKeHl5Yfny5ahevToAYPfu3XByctKYJydEmpqa5rnswMBA/PPPP5g7dy5cXFygVCrRsGHDfF9IkZ2dLa0jMzNT40Pc2NhY+pJRq9WIi4vD6NGjNebPOf3l6ekJlUqFgQMHYuTIkRof9ET0fmvRogUWLlwIY2NjlC1bFiVK/Pu1/vJnDvDiDgA+Pj5Yt26d1nJKly4NA4OCHd+qXbs2EhMTsXfvXhw8eBA9e/aEn58ftmzZUrgXA2gdXFAoFFCr1QA0P1Nz7u7y8mu0sLCQAuzZs2e1hkdYW1vD2toaFStWRIMGDVCyZEls374dvXv3LnS99HbgkVl6ZxgYGOCbb77B2LFj4enpCaVSiZs3b8LDw0PjkRMSa9asiePHj+d6y6uTJ09i2LBhaN++PapVqwalUomUlJR81+Ph4YELFy4AAJo0aYL9+/fjl19+gUqlwvz58/Ho0SOkpaVh5MiRcHJyQt26dXNdllqtRlZWlvShTkQE/Psf+vLly2sEWV1q166NK1euwN7eXutz0draGpaWlnB1dcWhQ4fyvX4rKysEBARg6dKliIqKwtatW/HgwQOtflWrVsX58+fx5MkTqe3kyZMwMDBA5cqV87WuChUq4PLly8jKyoKNjQ2qVauGqVOnIisrC5cuXcLGjRuhVquxe/duLFiwQGs87suEEBBCICMjI9+vld5eDLP0TunRowcMDQ2xePFijBo1CiNGjMDq1atx9epVnD17FvPmzcPq1asBAEOHDkVaWhp69eqF3377DVeuXMGaNWukU14VK1bEmjVrcPHiRZw+fRp9+vR55dHcl3Xu3BmbN2/GgwcPUKdOHYSEhMDX1xdKpRL79++Hj48PevXqhYcPH2L79u3SfOvWrcOmTZtw8eJFXLt2DZs2bUJoaCgCAgJyHRJBRPQqffr0gZ2dHbp06YLjx48jMTERR44cwbBhw/D3338DACZOnIjw8HB8//33uHLlivS5qUtERAQ2bNiAS5cu4fLly9i8eTMcHR1hY2Ojc90mJiYIDAzEH3/8gZ9//hlffvklPvnkEzg4OOSrfjs7O9SsWVO6sGvlypXYsGEDTE1N4efnh86dO2Pt2rUYP348Nm3ahKpVqwIArl27hrCwMMTGxuLmzZs4deoUevToAVNTU7Rv374QW5LeNhxmQO+UEiVKYOjQoZg5cyYSExNRunRphIWF4dq1a7CxsUHt2rXxzTffAABKlSqFw4cPY/To0WjWrBkMDQ3h7e2Nxo0bAwCWL1+OgQMHonbt2nB2dsa0adMwatSofNfi4eGBHj16oHfv3ti+fTvGjRuHUaNGIT09Hfb29rh37x5sbGw0xrTlvIYZM2bg8uXLEELAxcUFQ4cOxYgRI4puQxHRe8fMzAzHjh3DmDFj0K1bN6Snp8PJyQmtWrWClZUVgBfDq54/f445c+Zg1KhRsLOzw4cffqhzeZaWlpg5cyauXLkCQ0ND1K1bF3v27NE5XMHMzAz79u3D8OHDUbduXZiZmaF79+6IiIgo0GsICwtDp06d4OXlhbp16+LmzZu4e/cu7O3t8fz5c8yYMUMrTJuYmOD48eOIjIzEw4cP4eDggKZNm+LUqVOwt7cv0Prp7aQQQgh9F0H0rsrMzESPHj1w5coVjB8/Hu3atYO1tTUePXqEbdu2ISIiAtHR0ShXrpy+SyUikoXVq1dj+PDhGDZsGPr27Qt3d3eoVCqcOXMGYWFhaNmyJf/z/55hmCV6w4QQWL16NebOnYu4uDgYGxtDrVbD19cXY8eORcuWLfVdIhGRrPz++++YPHky9u7di8zMTKjVari4uGDQoEEYMWKE1hkvercxzBIVo8ePH+PBgwcoXbp0gcbfEhGRtuzsbCQnJ0OpVMLOzk7f5ZCeMMwSERERkWzxbgZEREREJFsMs0REREQkWwyzRERERCRbDLNEREREJFsMs0REREQkWwyzRERERCRbDLNEREREJFsMs0REREQkWwyzRERERCRb/wcMkxAaSfjtrgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple bar plot to visualize average metrics\n",
    "avg_kw_recall = comparison[\"kw_recall@3\"].mean()\n",
    "avg_tf_recall = comparison[\"tfidf_recall@3\"].mean()\n",
    "avg_kw_prec = comparison[\"kw_precision@3\"].mean()\n",
    "avg_tf_prec = comparison[\"tfidf_precision@3\"].mean()\n",
    "\n",
    "metrics = [\"Recall@3\", \"Precision@3\"]\n",
    "kw_scores = [avg_kw_recall, avg_kw_prec]\n",
    "tf_scores = [avg_tf_recall, avg_tf_prec]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, kw_scores, width, label=\"Keyword\")\n",
    "plt.bar(x + width/2, tf_scores, width, label=\"TF-IDF\")\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Keyword vs TF-IDF Retrieval (Average over Queries)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88501a4d",
   "metadata": {},
   "source": [
    "## 5. Answer Quality: A Simple Overlap Metric\n",
    "\n",
    "Once retrieval is working, we also care about **how good the answers are**.\n",
    "\n",
    "In production, people use more advanced metrics like:\n",
    "\n",
    "- **BLEU / ROUGE / METEOR** ‚Üí compare wording and phrasing\n",
    "- **BERTScore / MoverScore** ‚Üí compare meaning using embeddings\n",
    "- **LLM-as-a-judge** ‚Üí A powerful LLM scores the answer on a 1‚Äì10 scale\n",
    "\n",
    "For this tutorial, we‚Äôll use a **super simple metric**:\n",
    "\n",
    "> **‚ÄúWhat fraction of important words in the gold answer also appear in the model‚Äôs answer?‚Äù**\n",
    "\n",
    "This is **NOT** a production-grade metric, but it is:\n",
    "\n",
    "- Easy to understand\n",
    "- Easy to implement\n",
    "- A good starting point to think about ‚Äúanswer overlap‚Äù and ‚Äúcoverage‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc20f6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>overlap_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  overlap_score\n",
       "0         0       0.166667\n",
       "1         1       0.166667\n",
       "2         2       0.384615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"and\", \"or\", \"to\", \"in\", \"on\",\n",
    "    \"for\", \"with\", \"that\", \"this\", \"it\", \"as\", \"by\", \"be\"\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> List[str]:\n",
    "    \"\"\"Lowercase, remove punctuation, split into words, drop stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def overlap_score(gold: str, pred: str) -> float:\n",
    "    \"\"\"Fraction of gold words that appear in the predicted answer.\n",
    "    \n",
    "    score = |gold_words ‚à© pred_words| / |gold_words|\n",
    "    \"\"\"\n",
    "    gold_tokens = set(normalize_text(gold))\n",
    "    pred_tokens = set(normalize_text(pred))\n",
    "    if not gold_tokens:\n",
    "        return 0.0\n",
    "    overlap = gold_tokens & pred_tokens\n",
    "    return len(overlap) / len(gold_tokens)\n",
    "\n",
    "\n",
    "# Demo on our tiny set\n",
    "rows = []\n",
    "for qi in range(len(queries)):\n",
    "    gold = gold_answers[qi]\n",
    "    pred = model_answers[qi]\n",
    "    score = overlap_score(gold, pred)\n",
    "    rows.append({\n",
    "        \"query_id\": qi,\n",
    "        \"query\": queries[qi],\n",
    "        \"gold_answer\": gold,\n",
    "        \"model_answer\": pred,\n",
    "        \"overlap_score\": score,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)[[\"query_id\", \"overlap_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd08dbe",
   "metadata": {},
   "source": [
    "### 5.1 Inspecting One Example\n",
    "\n",
    "Let‚Äôs look at the **RAG** question and see which words overlap between the gold and model answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2e00b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "How do RAG systems work?\n",
      "\n",
      "GOLD ANSWER:\n",
      "A RAG system retrieves relevant documents and feeds them into an LLM to generate grounded answers.\n",
      "\n",
      "MODEL ANSWER:\n",
      "RAG uses search plus a language model. It looks up context and then the LLM uses it to answer.\n",
      "\n",
      "Important words in gold answer: {'grounded', 'into', 'documents', 'them', 'generate', 'answers', 'rag', 'feeds', 'llm', 'retrieves', 'system', 'relevant'}\n",
      "Words used by model: {'context', 'rag', 'plus', 'looks', 'search', 'model', 'up', 'llm', 'answer', 'then', 'language', 'uses'}\n",
      "Overlap: {'rag', 'llm'}\n",
      "Overlap score: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "example_q = 0\n",
    "gold = gold_answers[example_q]\n",
    "pred = model_answers[example_q]\n",
    "\n",
    "print(\"QUESTION:\")\n",
    "print(queries[example_q])\n",
    "print(\"\\nGOLD ANSWER:\")\n",
    "print(gold)\n",
    "print(\"\\nMODEL ANSWER:\")\n",
    "print(pred)\n",
    "\n",
    "gold_tokens = set(normalize_text(gold))\n",
    "pred_tokens = set(normalize_text(pred))\n",
    "print(\"\\nImportant words in gold answer:\", gold_tokens)\n",
    "print(\"Words used by model:\", pred_tokens)\n",
    "print(\"Overlap:\", gold_tokens & pred_tokens)\n",
    "print(\"Overlap score:\", overlap_score(gold, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a07e906",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Build a tiny retrieval dataset** with documents, queries, and ground-truth relevance.\n",
    "2. Implement **Recall@k** and **Precision@k** to measure retrieval quality.\n",
    "3. Compare a simple **keyword overlap retriever** to a classic **TF‚ÄëIDF + cosine similarity** retriever.\n",
    "4. Compute a **simple answer-overlap score** to get a first signal of answer quality.\n",
    "\n",
    "### Where to go next\n",
    "\n",
    "- Swap TF‚ÄëIDF for **embeddings** (e.g., OpenAI, SentenceTransformers) and re-run the same metrics.\n",
    "- Replace the simple overlap score with:\n",
    "  - ROUGE or BERTScore\n",
    "  - LLM-as-a-judge scoring (e.g., GPT comparing two answers)\n",
    "- Log real **user queries + feedback** and treat this notebook as a template for your own evaluation stack.\n",
    "\n",
    "> üß© **Key idea:** RAG / LLM evaluation is not magic.  \n",
    "> It‚Äôs mostly about **clearly defining what ‚Äúgood‚Äù means**, then turning that into **measurable metrics** you can compute and track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99937751",
   "metadata": {},
   "source": [
    "## Advanced Evaluation Metrics (Beginner Overview)\n",
    "\n",
    "### **BLEU, ROUGE, METEOR ‚Äî Text Overlap Metrics**\n",
    "- BLEU ‚Üí used in machine translation\n",
    "- ROUGE ‚Üí used in summarization\n",
    "- METEOR ‚Üí considers synonyms + ordering\n",
    "\n",
    "**Beginner intuition:** These metrics detect *how much wording is shared* between prediction and reference.\n",
    "\n",
    "---\n",
    "### **BERTScore, MoverScore ‚Äî Semantic Similarity Metrics**\n",
    "- Use embeddings to measure *meaning*, not exact words\n",
    "- Much better for paraphrased answers\n",
    "\n",
    "**Beginner intuition:** Even if words differ, these metrics ask: *Do the answers mean the same thing?*\n",
    "\n",
    "---\n",
    "### **LLM‚Äëas‚Äëa‚ÄëJudge ‚Äî Model‚ÄëGraded Evaluation**\n",
    "A large model (GPT‚Äë4, GPT‚Äë5, etc.) evaluates an answer.\n",
    "\n",
    "Example prompt:\n",
    "> ‚ÄúScore this answer from 1‚Äì10 based on correctness and helpfulness.‚Äù\n",
    "\n",
    "**Beginner intuition:** Instead of rigid text rules, we ask a model to evaluate like a human.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674795e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_overlap(reference: str, prediction: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple text‚Äëoverlap metric for beginners.\n",
    "    Extracts meaningful words and measures how many appear in both.\n",
    "    \"\"\"\n",
    "    ref_words = set(re.findall(r\"[a-zA-Z]+\", reference.lower()))\n",
    "    pred_words = set(re.findall(r\"[a-zA-Z]+\", prediction.lower()))\n",
    "\n",
    "    if not ref_words:\n",
    "        return 0.0\n",
    "\n",
    "    overlap = ref_words.intersection(pred_words)\n",
    "    return len(overlap) / len(ref_words)\n",
    "\n",
    "# Example\n",
    "ref = \"Paris is the capital of France.\"\n",
    "pred = \"The capital city of France is Paris.\"\n",
    "print(simple_overlap(ref, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
